**üéôÔ∏è 30 Days of Voice Agents Challenge** 

Welcome to my repository for the 30 Days of Voice Agents challenge, organized by Murf AI. This challenge is all about building voice-powered apps, experimenting with AI APIs, and leveling up with tools like FastAPI and JavaScript over 30 days.

**What is this Challenge?** 

The goal is to create 30 mini voice agent projects in 30 days. Each day focuses on a new concept‚Äîfrom UI interactions to speech synthesis and voice APIs.

**Repository Structure**

- main.py ‚Äì Backend logic (FastAPI)
- templates/ ‚Äì Jinja2 HTML files
- static/ ‚Äì JS or CSS assets
- requirements.txt ‚Äì Dependencies
- README.md ‚Äì Specifics for that day‚Äôs project

**Technologies Used**

- Python
- FastAPI
- requests, python-dotenv, uvicorn
- Frontend
- HTML / CSS / Bootstrap
- JavaScript (including the MediaRecorder API)
- AI & Voice Tools
- Murf AI API for Text-to-Speech (TTS)
- AssemblyAI API for Speech-to-Text (STT)

**Running a Project**

- cd day-##/
- pip install -r requirements.txt
- uvicorn main:app --reload
- http://127.0.0.1:8000.

**üß† Environment Variables  **<br>
MURF_API_KEY ‚Äî Your Murf API key (get from https://murf.ai) <br>
ASSEMBLYAI_API_KEY ‚Äî Your AssemblyAI API key (get from https://www.assemblyai.com)<br>
GEMINI_API_KEY or GOOGLE_API_KEY ‚Äî Your Google Gemini API key (get from https://aistudio.google.com/app/apikey) <br>


**Completed Days**

- Day 01: Set up a basic FastAPI server with a Bootstrap UI. <br>
- Day 02: Simple Text-to-Speech (TTS) using the Murf AI API. <br>
- Day 03: Added client-side audio recording using the MediaRecorder API. <br>
- Day 04: Sent recorded audio from the frontend to the FastAPI backend. <br>
- Day 05: Transcribed uploaded audio using the AssemblyAI API. <br>
- Day 06: Integrated the TTS and STT features into a single, organized UI. <br>
- Day 07: Created an "Echo Bot" that transcribes user audio and speaks it back in a new voice. <br>
- Day 08: Introduced intelligence by integrating the Google Gemini LLM, creating an endpoint that could generate text-based responses to queries.<br>
- Day 09: Achieved a full voice-to-voice conversational loop. The app could now listen to a spoken question and provide a spoken answer generated by the LLM.<br>
- Day 10: Implemented chat history and session management, giving the agent a "memory" to hold context-aware conversations.<br>
- Day 11: Made the application more robust by adding server-side and client-side error handling, including a friendly fallback audio message for API failures.<br>
- Day 12: Performed a major UI revamp, simplifying the interface to a single, animated record button and a cleaner, more modern aesthetic.<br>
- Day 13: Focused on documentation, creating this comprehensive README.md file to explain the project's architecture, features, and setup.<br>
- Day 14: Refactor the code to make it more readable and maintainable and Clean up the code by removing unused imports, variables, and functions and Upload the       code to Github.<br>
- Day 15: Added a foundational WebSocket endpoint to the server.<br>
- Day 16: Implemented real-time audio streaming from the client using WebSockets.<br>

**Stay tuned for more!**
